{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900b9343-7ef9-4cf7-aa76-1eb57270ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f9da466-b63c-47bd-b349-0fd225062a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/te/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/te/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/te/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/te/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/te/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/te/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84f5b64f-9b62-4c89-a033-034c8a8369c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    tokenized_text = sent_tokenize(text)\n",
    "    print(\"\\nSentence Tokenization: \",tokenized_text)\n",
    "\n",
    "    tokenized_word = word_tokenize(text)\n",
    "    print(\"\\nWord Tokenization: \", tokenized_word)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(\"\\nStop Words: \", stop_words)\n",
    "\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    tokenized_word = word_tokenize(text.lower())\n",
    "    print(\"\\nNew word tokenization by Punctuation removed:\\n\", tokenized_word)\n",
    "    filtered_text = []\n",
    "\n",
    "    for w in tokenized_word:\n",
    "        if w not in stop_words:\n",
    "            filtered_text.append(w)\n",
    "\n",
    "    print(\"\\nFiltered Text:\", filtered_text)\n",
    "\n",
    "    print(\"\\n\\nPOS Tagging on filtered text: \\n\")\n",
    "    print(\"\"\" \n",
    "        Comman Tags:\\n\n",
    "NN: Noun, singular or mass.\n",
    "NNS: Noun, plural.\n",
    "JJ: Adjective.\n",
    "JJR/JJS: Adjective, comparative/superlative.\n",
    "RB: Adverb.\n",
    "VBZ: Verb, 3rd person singular present (e.g., \"is\", \"jumps\").\n",
    "VBD: Verb, past tense.\n",
    "DT: Determiner.\n",
    "IN: Preposition or subordinating conjunction. \n",
    "    \"\"\")\n",
    "    for word in filtered_text:\n",
    "        print(nltk.pos_tag([word]))\n",
    "\n",
    "    \"\"\"\n",
    "    Stemming: in Natural Language Processing (NLP) is a text normalization technique that reduces inflected\n",
    "    or derived words to their base or root form (e.g., \"running\", \"runner\", and \"ran\" all become \"run\"). \n",
    "    Drawbacks: Stemming can produce non-words (e.g., \"studies\" might become \"studi\") and is less accurate than lemmatization, \n",
    "    which uses vocabulary and morphological analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ps =PorterStemmer()\n",
    "    stem_text = []\n",
    "    print(\"\\nStemming: \\n\")\n",
    "    for w in filtered_text:\n",
    "        stem_text.append(ps.stem(w))\n",
    "\n",
    "    print(\"\\nFiltered Text:\", filtered_text)\n",
    "    print(\"\\n\\nStemming of Text: \\n\")\n",
    "    print(stem_text)\n",
    "\n",
    "    \"\"\"\n",
    "    Lemmatization in Natural Language Processing (NLP) is a text pre-processing technique that reduces words \n",
    "    to their base or dictionary form, known as a lemma. \n",
    "    \"\"\"\n",
    "\n",
    "    word_lem = WordNetLemmatizer()\n",
    "    lemma_text = []\n",
    "    # print(\"\\nLemmatization: \\n\")\n",
    "    for w in filtered_text:\n",
    "        lemma_text.append(word_lem.lemmatize(w))\n",
    "\n",
    "    # print(\"\\nFiltered Text:\", filtered_text)\n",
    "    print(\"\\n\\nLemmatization of Text: \\n\")\n",
    "    print(lemma_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc72d35e-a37c-4fec-9395-1f8a4c3003db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokenization:  ['Natural Language Processing (NLP) is a fascinating subfield of artificial intelligence, computer science, and linguistics aimed at enabling computers to understand, interpret, and generate human language in a meaningful way.', \"Techniques such as tokenization, stemming, lemmatization, and stop-word removal are often employed to preprocess text data—whether it's messy social media posts, unstructured medical records, or large-scale document repositories—before feeding it into machine learning models.\", \"For instance, in sentiment analysis, a system might analyze a product review like 'The product is AMAZING!\", \"However, the shipping took 3 weeks...' and identify conflicting emotional tones (positive product perception vs. negative logistics experience).\", 'As we advance, libraries such as spaCy and NLTK are crucial, particularly when tackling tasks like Named Entity Recognition (NER), which identifies entities (e.g., Apple, London, 2024), or Part-of-Speech (POS) tagging.', \"Furthermore, it is important to consider that context is everything: a word like 'bank' can represent a financial institution or the side of a river, creating semantic ambiguity that requires advanced transformer-based models (like BERT or GPT) to resolve properly.\", 'Consequently, text mining and extraction, when combined with semantic analysis, allow organizations to turn massive, unorganized text datasets into actionable insights or, at the very least, improve their spam filters and chatbots']\n",
      "\n",
      "Word Tokenization:  ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'subfield', 'of', 'artificial', 'intelligence', ',', 'computer', 'science', ',', 'and', 'linguistics', 'aimed', 'at', 'enabling', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'meaningful', 'way', '.', 'Techniques', 'such', 'as', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'and', 'stop-word', 'removal', 'are', 'often', 'employed', 'to', 'preprocess', 'text', 'data—whether', 'it', \"'s\", 'messy', 'social', 'media', 'posts', ',', 'unstructured', 'medical', 'records', ',', 'or', 'large-scale', 'document', 'repositories—before', 'feeding', 'it', 'into', 'machine', 'learning', 'models', '.', 'For', 'instance', ',', 'in', 'sentiment', 'analysis', ',', 'a', 'system', 'might', 'analyze', 'a', 'product', 'review', 'like', \"'The\", 'product', 'is', 'AMAZING', '!', 'However', ',', 'the', 'shipping', 'took', '3', 'weeks', '...', \"'\", 'and', 'identify', 'conflicting', 'emotional', 'tones', '(', 'positive', 'product', 'perception', 'vs.', 'negative', 'logistics', 'experience', ')', '.', 'As', 'we', 'advance', ',', 'libraries', 'such', 'as', 'spaCy', 'and', 'NLTK', 'are', 'crucial', ',', 'particularly', 'when', 'tackling', 'tasks', 'like', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', ',', 'which', 'identifies', 'entities', '(', 'e.g.', ',', 'Apple', ',', 'London', ',', '2024', ')', ',', 'or', 'Part-of-Speech', '(', 'POS', ')', 'tagging', '.', 'Furthermore', ',', 'it', 'is', 'important', 'to', 'consider', 'that', 'context', 'is', 'everything', ':', 'a', 'word', 'like', \"'bank\", \"'\", 'can', 'represent', 'a', 'financial', 'institution', 'or', 'the', 'side', 'of', 'a', 'river', ',', 'creating', 'semantic', 'ambiguity', 'that', 'requires', 'advanced', 'transformer-based', 'models', '(', 'like', 'BERT', 'or', 'GPT', ')', 'to', 'resolve', 'properly', '.', 'Consequently', ',', 'text', 'mining', 'and', 'extraction', ',', 'when', 'combined', 'with', 'semantic', 'analysis', ',', 'allow', 'organizations', 'to', 'turn', 'massive', ',', 'unorganized', 'text', 'datasets', 'into', 'actionable', 'insights', 'or', ',', 'at', 'the', 'very', 'least', ',', 'improve', 'their', 'spam', 'filters', 'and', 'chatbots']\n",
      "\n",
      "Stop Words:  {'by', \"don't\", \"i'll\", 'of', \"we've\", 'each', 'him', \"it'll\", 'who', 'about', 'do', 'herself', \"hasn't\", \"needn't\", \"they'll\", 's', 'there', \"doesn't\", 'for', \"mustn't\", 'has', 'having', \"isn't\", \"you've\", 'during', 'how', \"you'll\", 'and', 'again', 'off', 'wasn', 'ourselves', 'hers', 'should', 'those', \"it'd\", \"couldn't\", \"that'll\", 'than', 'only', 'am', 'shouldn', \"they're\", 'yourselves', 're', 'where', 'no', 'an', 'ma', 'it', 'll', 'her', \"he'd\", 'y', 'such', 'you', 'both', 'while', 'yourself', 'them', 'i', 'not', \"shan't\", \"she's\", \"you'd\", 'ain', 'down', 'above', 'couldn', \"she'd\", 'she', \"should've\", \"won't\", 'any', 'over', 'are', 'as', 'all', \"weren't\", \"you're\", 'just', \"i'm\", \"they'd\", 'been', 'why', 'these', 'when', 'against', \"i'd\", 'wouldn', 'his', 'our', \"wouldn't\", 'before', 'is', 'between', 'o', \"we'd\", \"hadn't\", 'in', 'to', \"she'll\", \"he'll\", 'hasn', 'weren', 'itself', 'the', 'up', 'they', 'at', 'does', 'into', 'if', \"mightn't\", 'more', 'theirs', 'most', 'aren', 'because', 'doesn', 'themselves', 'my', 'nor', 'that', 'won', 'will', 'this', 'he', 'once', 'did', 'haven', 'further', 'had', 'what', 'd', 'didn', \"shouldn't\", 'from', 'or', 'whom', \"haven't\", 'was', 'other', 'some', 't', \"didn't\", 'their', 'then', 'but', 'being', 'on', 'so', 'now', 'needn', 'out', 'be', 'mightn', 'shan', 'through', 'we', 'doing', 'm', 'very', 'yours', 'own', \"we're\", 'under', 'hadn', 've', 'were', 'few', 'after', \"aren't\", 'can', 'too', 'your', 'ours', 'its', \"we'll\", \"they've\", 'me', 'which', 'with', \"he's\", \"it's\", 'mustn', 'same', 'until', 'himself', 'below', 'isn', 'myself', 'have', 'a', 'don', \"wasn't\", \"i've\", 'here'}\n",
      "\n",
      "New word tokenization by Punctuation removed:\n",
      " ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'subfield', 'of', 'artificial', 'intelligence', 'computer', 'science', 'and', 'linguistics', 'aimed', 'at', 'enabling', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'meaningful', 'way', 'techniques', 'such', 'as', 'tokenization', 'stemming', 'lemmatization', 'and', 'stop', 'word', 'removal', 'are', 'often', 'employed', 'to', 'preprocess', 'text', 'data', 'whether', 'it', 's', 'messy', 'social', 'media', 'posts', 'unstructured', 'medical', 'records', 'or', 'large', 'scale', 'document', 'repositories', 'before', 'feeding', 'it', 'into', 'machine', 'learning', 'models', 'for', 'instance', 'in', 'sentiment', 'analysis', 'a', 'system', 'might', 'analyze', 'a', 'product', 'review', 'like', 'the', 'product', 'is', 'amazing', 'however', 'the', 'shipping', 'took', 'weeks', 'and', 'identify', 'conflicting', 'emotional', 'tones', 'positive', 'product', 'perception', 'vs', 'negative', 'logistics', 'experience', 'as', 'we', 'advance', 'libraries', 'such', 'as', 'spacy', 'and', 'nltk', 'are', 'crucial', 'particularly', 'when', 'tackling', 'tasks', 'like', 'named', 'entity', 'recognition', 'ner', 'which', 'identifies', 'entities', 'e', 'g', 'apple', 'london', 'or', 'part', 'of', 'speech', 'pos', 'tagging', 'furthermore', 'it', 'is', 'important', 'to', 'consider', 'that', 'context', 'is', 'everything', 'a', 'word', 'like', 'bank', 'can', 'represent', 'a', 'financial', 'institution', 'or', 'the', 'side', 'of', 'a', 'river', 'creating', 'semantic', 'ambiguity', 'that', 'requires', 'advanced', 'transformer', 'based', 'models', 'like', 'bert', 'or', 'gpt', 'to', 'resolve', 'properly', 'consequently', 'text', 'mining', 'and', 'extraction', 'when', 'combined', 'with', 'semantic', 'analysis', 'allow', 'organizations', 'to', 'turn', 'massive', 'unorganized', 'text', 'datasets', 'into', 'actionable', 'insights', 'or', 'at', 'the', 'very', 'least', 'improve', 'their', 'spam', 'filters', 'and', 'chatbots']\n",
      "\n",
      "Filtered Text: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'subfield', 'artificial', 'intelligence', 'computer', 'science', 'linguistics', 'aimed', 'enabling', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'meaningful', 'way', 'techniques', 'tokenization', 'stemming', 'lemmatization', 'stop', 'word', 'removal', 'often', 'employed', 'preprocess', 'text', 'data', 'whether', 'messy', 'social', 'media', 'posts', 'unstructured', 'medical', 'records', 'large', 'scale', 'document', 'repositories', 'feeding', 'machine', 'learning', 'models', 'instance', 'sentiment', 'analysis', 'system', 'might', 'analyze', 'product', 'review', 'like', 'product', 'amazing', 'however', 'shipping', 'took', 'weeks', 'identify', 'conflicting', 'emotional', 'tones', 'positive', 'product', 'perception', 'vs', 'negative', 'logistics', 'experience', 'advance', 'libraries', 'spacy', 'nltk', 'crucial', 'particularly', 'tackling', 'tasks', 'like', 'named', 'entity', 'recognition', 'ner', 'identifies', 'entities', 'e', 'g', 'apple', 'london', 'part', 'speech', 'pos', 'tagging', 'furthermore', 'important', 'consider', 'context', 'everything', 'word', 'like', 'bank', 'represent', 'financial', 'institution', 'side', 'river', 'creating', 'semantic', 'ambiguity', 'requires', 'advanced', 'transformer', 'based', 'models', 'like', 'bert', 'gpt', 'resolve', 'properly', 'consequently', 'text', 'mining', 'extraction', 'combined', 'semantic', 'analysis', 'allow', 'organizations', 'turn', 'massive', 'unorganized', 'text', 'datasets', 'actionable', 'insights', 'least', 'improve', 'spam', 'filters', 'chatbots']\n",
      "\n",
      "\n",
      "POS Tagging on filtered text: \n",
      "\n",
      " \n",
      "        Comman Tags:\n",
      "\n",
      "NN: Noun, singular or mass.\n",
      "NNS: Noun, plural.\n",
      "JJ: Adjective.\n",
      "JJR/JJS: Adjective, comparative/superlative.\n",
      "RB: Adverb.\n",
      "VBZ: Verb, 3rd person singular present (e.g., \"is\", \"jumps\").\n",
      "VBD: Verb, past tense.\n",
      "DT: Determiner.\n",
      "IN: Preposition or subordinating conjunction. \n",
      "    \n",
      "[('natural', 'JJ')]\n",
      "[('language', 'NN')]\n",
      "[('processing', 'NN')]\n",
      "[('nlp', 'NN')]\n",
      "[('fascinating', 'VBG')]\n",
      "[('subfield', 'NN')]\n",
      "[('artificial', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('computer', 'NN')]\n",
      "[('science', 'NN')]\n",
      "[('linguistics', 'NNS')]\n",
      "[('aimed', 'VBN')]\n",
      "[('enabling', 'VBG')]\n",
      "[('computers', 'NNS')]\n",
      "[('understand', 'NN')]\n",
      "[('interpret', 'NN')]\n",
      "[('generate', 'NN')]\n",
      "[('human', 'NN')]\n",
      "[('language', 'NN')]\n",
      "[('meaningful', 'NN')]\n",
      "[('way', 'NN')]\n",
      "[('techniques', 'NNS')]\n",
      "[('tokenization', 'NN')]\n",
      "[('stemming', 'VBG')]\n",
      "[('lemmatization', 'NN')]\n",
      "[('stop', 'NN')]\n",
      "[('word', 'NN')]\n",
      "[('removal', 'NN')]\n",
      "[('often', 'RB')]\n",
      "[('employed', 'VBN')]\n",
      "[('preprocess', 'NN')]\n",
      "[('text', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('whether', 'IN')]\n",
      "[('messy', 'NN')]\n",
      "[('social', 'JJ')]\n",
      "[('media', 'NNS')]\n",
      "[('posts', 'NNS')]\n",
      "[('unstructured', 'JJ')]\n",
      "[('medical', 'JJ')]\n",
      "[('records', 'NNS')]\n",
      "[('large', 'JJ')]\n",
      "[('scale', 'NN')]\n",
      "[('document', 'NN')]\n",
      "[('repositories', 'NNS')]\n",
      "[('feeding', 'VBG')]\n",
      "[('machine', 'NN')]\n",
      "[('learning', 'VBG')]\n",
      "[('models', 'NNS')]\n",
      "[('instance', 'NN')]\n",
      "[('sentiment', 'NN')]\n",
      "[('analysis', 'NN')]\n",
      "[('system', 'NN')]\n",
      "[('might', 'MD')]\n",
      "[('analyze', 'NN')]\n",
      "[('product', 'NN')]\n",
      "[('review', 'NN')]\n",
      "[('like', 'IN')]\n",
      "[('product', 'NN')]\n",
      "[('amazing', 'VBG')]\n",
      "[('however', 'RB')]\n",
      "[('shipping', 'NN')]\n",
      "[('took', 'VBD')]\n",
      "[('weeks', 'NNS')]\n",
      "[('identify', 'VB')]\n",
      "[('conflicting', 'VBG')]\n",
      "[('emotional', 'JJ')]\n",
      "[('tones', 'NNS')]\n",
      "[('positive', 'JJ')]\n",
      "[('product', 'NN')]\n",
      "[('perception', 'NN')]\n",
      "[('vs', 'NN')]\n",
      "[('negative', 'JJ')]\n",
      "[('logistics', 'NNS')]\n",
      "[('experience', 'NN')]\n",
      "[('advance', 'NN')]\n",
      "[('libraries', 'NNS')]\n",
      "[('spacy', 'NN')]\n",
      "[('nltk', 'NN')]\n",
      "[('crucial', 'JJ')]\n",
      "[('particularly', 'RB')]\n",
      "[('tackling', 'VBG')]\n",
      "[('tasks', 'NNS')]\n",
      "[('like', 'IN')]\n",
      "[('named', 'VBN')]\n",
      "[('entity', 'NN')]\n",
      "[('recognition', 'NN')]\n",
      "[('ner', 'NN')]\n",
      "[('identifies', 'NNS')]\n",
      "[('entities', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('apple', 'NN')]\n",
      "[('london', 'NN')]\n",
      "[('part', 'NN')]\n",
      "[('speech', 'NN')]\n",
      "[('pos', 'NN')]\n",
      "[('tagging', 'VBG')]\n",
      "[('furthermore', 'RB')]\n",
      "[('important', 'JJ')]\n",
      "[('consider', 'VB')]\n",
      "[('context', 'NN')]\n",
      "[('everything', 'NN')]\n",
      "[('word', 'NN')]\n",
      "[('like', 'IN')]\n",
      "[('bank', 'NN')]\n",
      "[('represent', 'NN')]\n",
      "[('financial', 'JJ')]\n",
      "[('institution', 'NN')]\n",
      "[('side', 'NN')]\n",
      "[('river', 'NN')]\n",
      "[('creating', 'VBG')]\n",
      "[('semantic', 'JJ')]\n",
      "[('ambiguity', 'NN')]\n",
      "[('requires', 'VBZ')]\n",
      "[('advanced', 'JJ')]\n",
      "[('transformer', 'NN')]\n",
      "[('based', 'VBN')]\n",
      "[('models', 'NNS')]\n",
      "[('like', 'IN')]\n",
      "[('bert', 'NN')]\n",
      "[('gpt', 'NN')]\n",
      "[('resolve', 'NN')]\n",
      "[('properly', 'RB')]\n",
      "[('consequently', 'RB')]\n",
      "[('text', 'NN')]\n",
      "[('mining', 'NN')]\n",
      "[('extraction', 'NN')]\n",
      "[('combined', 'VBN')]\n",
      "[('semantic', 'JJ')]\n",
      "[('analysis', 'NN')]\n",
      "[('allow', 'VB')]\n",
      "[('organizations', 'NNS')]\n",
      "[('turn', 'NN')]\n",
      "[('massive', 'JJ')]\n",
      "[('unorganized', 'JJ')]\n",
      "[('text', 'NN')]\n",
      "[('datasets', 'NNS')]\n",
      "[('actionable', 'JJ')]\n",
      "[('insights', 'NNS')]\n",
      "[('least', 'JJS')]\n",
      "[('improve', 'VB')]\n",
      "[('spam', 'NN')]\n",
      "[('filters', 'NNS')]\n",
      "[('chatbots', 'NNS')]\n",
      "\n",
      "Stemming: \n",
      "\n",
      "\n",
      "Filtered Text: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'subfield', 'artificial', 'intelligence', 'computer', 'science', 'linguistics', 'aimed', 'enabling', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'meaningful', 'way', 'techniques', 'tokenization', 'stemming', 'lemmatization', 'stop', 'word', 'removal', 'often', 'employed', 'preprocess', 'text', 'data', 'whether', 'messy', 'social', 'media', 'posts', 'unstructured', 'medical', 'records', 'large', 'scale', 'document', 'repositories', 'feeding', 'machine', 'learning', 'models', 'instance', 'sentiment', 'analysis', 'system', 'might', 'analyze', 'product', 'review', 'like', 'product', 'amazing', 'however', 'shipping', 'took', 'weeks', 'identify', 'conflicting', 'emotional', 'tones', 'positive', 'product', 'perception', 'vs', 'negative', 'logistics', 'experience', 'advance', 'libraries', 'spacy', 'nltk', 'crucial', 'particularly', 'tackling', 'tasks', 'like', 'named', 'entity', 'recognition', 'ner', 'identifies', 'entities', 'e', 'g', 'apple', 'london', 'part', 'speech', 'pos', 'tagging', 'furthermore', 'important', 'consider', 'context', 'everything', 'word', 'like', 'bank', 'represent', 'financial', 'institution', 'side', 'river', 'creating', 'semantic', 'ambiguity', 'requires', 'advanced', 'transformer', 'based', 'models', 'like', 'bert', 'gpt', 'resolve', 'properly', 'consequently', 'text', 'mining', 'extraction', 'combined', 'semantic', 'analysis', 'allow', 'organizations', 'turn', 'massive', 'unorganized', 'text', 'datasets', 'actionable', 'insights', 'least', 'improve', 'spam', 'filters', 'chatbots']\n",
      "\n",
      "\n",
      "Stemming of Text: \n",
      "\n",
      "['natur', 'languag', 'process', 'nlp', 'fascin', 'subfield', 'artifici', 'intellig', 'comput', 'scienc', 'linguist', 'aim', 'enabl', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'meaning', 'way', 'techniqu', 'token', 'stem', 'lemmat', 'stop', 'word', 'remov', 'often', 'employ', 'preprocess', 'text', 'data', 'whether', 'messi', 'social', 'media', 'post', 'unstructur', 'medic', 'record', 'larg', 'scale', 'document', 'repositori', 'feed', 'machin', 'learn', 'model', 'instanc', 'sentiment', 'analysi', 'system', 'might', 'analyz', 'product', 'review', 'like', 'product', 'amaz', 'howev', 'ship', 'took', 'week', 'identifi', 'conflict', 'emot', 'tone', 'posit', 'product', 'percept', 'vs', 'neg', 'logist', 'experi', 'advanc', 'librari', 'spaci', 'nltk', 'crucial', 'particularli', 'tackl', 'task', 'like', 'name', 'entiti', 'recognit', 'ner', 'identifi', 'entiti', 'e', 'g', 'appl', 'london', 'part', 'speech', 'po', 'tag', 'furthermor', 'import', 'consid', 'context', 'everyth', 'word', 'like', 'bank', 'repres', 'financi', 'institut', 'side', 'river', 'creat', 'semant', 'ambigu', 'requir', 'advanc', 'transform', 'base', 'model', 'like', 'bert', 'gpt', 'resolv', 'properli', 'consequ', 'text', 'mine', 'extract', 'combin', 'semant', 'analysi', 'allow', 'organ', 'turn', 'massiv', 'unorgan', 'text', 'dataset', 'action', 'insight', 'least', 'improv', 'spam', 'filter', 'chatbot']\n",
      "\n",
      "\n",
      "Lemmatization of Text: \n",
      "\n",
      "['natural', 'language', 'processing', 'nlp', 'fascinating', 'subfield', 'artificial', 'intelligence', 'computer', 'science', 'linguistics', 'aimed', 'enabling', 'computer', 'understand', 'interpret', 'generate', 'human', 'language', 'meaningful', 'way', 'technique', 'tokenization', 'stemming', 'lemmatization', 'stop', 'word', 'removal', 'often', 'employed', 'preprocess', 'text', 'data', 'whether', 'messy', 'social', 'medium', 'post', 'unstructured', 'medical', 'record', 'large', 'scale', 'document', 'repository', 'feeding', 'machine', 'learning', 'model', 'instance', 'sentiment', 'analysis', 'system', 'might', 'analyze', 'product', 'review', 'like', 'product', 'amazing', 'however', 'shipping', 'took', 'week', 'identify', 'conflicting', 'emotional', 'tone', 'positive', 'product', 'perception', 'v', 'negative', 'logistics', 'experience', 'advance', 'library', 'spacy', 'nltk', 'crucial', 'particularly', 'tackling', 'task', 'like', 'named', 'entity', 'recognition', 'ner', 'identifies', 'entity', 'e', 'g', 'apple', 'london', 'part', 'speech', 'po', 'tagging', 'furthermore', 'important', 'consider', 'context', 'everything', 'word', 'like', 'bank', 'represent', 'financial', 'institution', 'side', 'river', 'creating', 'semantic', 'ambiguity', 'requires', 'advanced', 'transformer', 'based', 'model', 'like', 'bert', 'gpt', 'resolve', 'properly', 'consequently', 'text', 'mining', 'extraction', 'combined', 'semantic', 'analysis', 'allow', 'organization', 'turn', 'massive', 'unorganized', 'text', 'datasets', 'actionable', 'insight', 'least', 'improve', 'spam', 'filter', 'chatbots']\n"
     ]
    }
   ],
   "source": [
    "# t = \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
    "file_name = \"Text_Processing_File\"\n",
    "para_text = open(file_name, \"r\")\n",
    "temp = \"\"\n",
    "for p in para_text:\n",
    "    temp = temp+p\n",
    "    # print(p)\n",
    "# preProcess(t)\n",
    "preProcess(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307c177-2579-4c36-8fd8-e20cead6f011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
